{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c88ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fb439c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0490ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09e9998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT Model has 199 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "embeddings.word_embeddings.weight                       (30522, 768)\n",
      "embeddings.position_embeddings.weight                     (512, 768)\n",
      "embeddings.token_type_embeddings.weight                     (2, 768)\n",
      "embeddings.LayerNorm.weight                                   (768,)\n",
      "embeddings.LayerNorm.bias                                     (768,)\n",
      "\n",
      "==== First Encoder ====\n",
      "\n",
      "encoder.layer.0.attention.self.query.weight               (768, 768)\n",
      "encoder.layer.0.attention.self.query.bias                     (768,)\n",
      "encoder.layer.0.attention.self.key.weight                 (768, 768)\n",
      "encoder.layer.0.attention.self.key.bias                       (768,)\n",
      "encoder.layer.0.attention.self.value.weight               (768, 768)\n",
      "encoder.layer.0.attention.self.value.bias                     (768,)\n",
      "encoder.layer.0.attention.output.dense.weight             (768, 768)\n",
      "encoder.layer.0.attention.output.dense.bias                   (768,)\n",
      "encoder.layer.0.attention.output.LayerNorm.weight             (768,)\n",
      "encoder.layer.0.attention.output.LayerNorm.bias               (768,)\n",
      "encoder.layer.0.intermediate.dense.weight                (3072, 768)\n",
      "encoder.layer.0.intermediate.dense.bias                      (3072,)\n",
      "encoder.layer.0.output.dense.weight                      (768, 3072)\n",
      "encoder.layer.0.output.dense.bias                             (768,)\n",
      "encoder.layer.0.output.LayerNorm.weight                       (768,)\n",
      "encoder.layer.0.output.LayerNorm.bias                         (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "pooler.dense.weight                                       (768, 768)\n",
      "pooler.dense.bias                                             (768,)\n"
     ]
    }
   ],
   "source": [
    "# Get all the model's parameters as a list of tuples\n",
    "\n",
    "named_params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT Model has {:} different named parameters.\\n'.format(len(named_params)))\n",
    "\n",
    "print(\"==== Embedding Layer ====\\n\")\n",
    "for p in named_params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "    \n",
    "print(\"\\n==== First Encoder ====\\n\")\n",
    "for p in named_params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "    \n",
    "print(\"\\n==== Output Layer ====\\n\")\n",
    "for p in named_params[-2:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aee9e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pooler is a separate linear and tanh activated layer that acts on the [CLS] token's representation\n",
    "# This pooled_output is often used as a representation for the entire sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a437d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e586e108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 8739, 2102, 7459, 1037, 3376, 2154, 102]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Saket loves a beautiful day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3767c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tokens through the model\n",
    "\n",
    "#1 Turn tokens_with_unknown_words into a tensor (will be shape(8,))\n",
    "#2 Unsqueeze a first dimension to simulate batches. Resulting shape is (1,8)\n",
    "\n",
    "response = model(torch.tensor(tokenizer.encode('Saket loves a beautiful day')).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1619975a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1778,  0.0946, -0.0306,  ..., -0.3201,  0.3875,  0.3578],\n",
       "         [ 0.6455,  0.4590,  0.4842,  ..., -0.5112,  0.7038, -0.3384],\n",
       "         [-0.0900, -0.4080,  0.7350,  ..., -1.0433, -0.1517, -0.2731],\n",
       "         ...,\n",
       "         [ 0.1477,  0.0171,  0.6067,  ..., -0.4519,  0.1851, -0.3535],\n",
       "         [-0.2638, -0.2562,  0.0473,  ...,  0.2247,  0.3425, -0.3819],\n",
       "         [ 0.5196, -0.0754, -0.2657,  ..., -0.0014, -0.4388, -0.2921]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.4523e-01, -4.1711e-01, -5.2523e-01,  7.2943e-01,  2.9464e-01,\n",
       "         -5.5997e-02,  8.9018e-01,  2.6216e-01, -2.0878e-01, -9.9997e-01,\n",
       "          2.2736e-02,  6.5921e-01,  9.9061e-01,  1.2573e-01,  9.5687e-01,\n",
       "         -4.3377e-01,  1.8056e-01, -5.5232e-01,  3.9684e-01, -4.9880e-01,\n",
       "          7.7684e-01,  9.9912e-01,  4.0945e-01,  2.0446e-01,  4.3359e-01,\n",
       "          9.1557e-01, -6.1338e-01,  9.4577e-01,  9.6313e-01,  7.2220e-01,\n",
       "         -5.6765e-01,  1.4873e-01, -9.9194e-01, -1.3043e-01, -7.2494e-01,\n",
       "         -9.9353e-01,  2.9261e-01, -7.2322e-01, -9.7168e-03,  5.2191e-02,\n",
       "         -8.9005e-01,  2.6919e-01,  9.9986e-01, -1.5671e-01,  2.4860e-01,\n",
       "         -2.1468e-01, -1.0000e+00,  2.7965e-01, -8.9067e-01,  4.4216e-01,\n",
       "          4.7319e-01,  2.6408e-01, -1.6957e-02,  4.4144e-01,  3.8046e-01,\n",
       "          2.7439e-01, -1.0210e-01,  9.6712e-02, -1.7693e-01, -5.5499e-01,\n",
       "         -5.8191e-01,  3.5610e-01, -5.7203e-01, -9.1834e-01,  3.5828e-01,\n",
       "          1.7463e-02, -1.3048e-01, -2.4343e-01,  4.8797e-02, -2.1625e-01,\n",
       "          8.3759e-01,  1.8995e-01,  1.1567e-01, -8.9225e-01, -4.1555e-02,\n",
       "          1.7064e-01, -5.1706e-01,  1.0000e+00, -2.4593e-01, -9.8421e-01,\n",
       "          3.5603e-01,  1.5931e-01,  4.4239e-01,  4.5921e-01, -1.6996e-01,\n",
       "         -1.0000e+00,  3.7562e-01, -5.7442e-02, -9.9203e-01,  1.4632e-01,\n",
       "          5.4373e-01, -1.3550e-01,  1.8196e-01,  4.6689e-01, -2.3914e-01,\n",
       "         -3.6202e-01, -2.5952e-01, -4.9641e-01, -2.9980e-01, -2.2290e-01,\n",
       "          5.4718e-02, -2.4404e-01, -1.8696e-01, -3.2008e-01,  1.8093e-01,\n",
       "         -3.7384e-01, -5.6698e-01,  2.4001e-01, -3.7898e-01,  6.7676e-01,\n",
       "          2.9100e-01, -2.9900e-01,  3.6774e-01, -9.5924e-01,  6.1999e-01,\n",
       "         -3.0044e-01, -9.8758e-01, -4.6834e-01, -9.9264e-01,  7.1138e-01,\n",
       "         -9.1191e-02, -2.3260e-01,  9.7396e-01, -4.7420e-03,  2.1159e-01,\n",
       "         -9.1473e-02, -3.7488e-01, -1.0000e+00, -5.2760e-01, -3.7645e-01,\n",
       "          1.9862e-01, -2.0135e-01, -9.8223e-01, -9.5724e-01,  5.4909e-01,\n",
       "          9.5322e-01,  1.4166e-01,  9.9975e-01, -3.5882e-01,  9.5929e-01,\n",
       "          1.1360e-01, -1.4832e-01,  2.3844e-01, -4.0256e-01,  6.3832e-01,\n",
       "          1.3579e-01, -6.1683e-01,  1.6516e-01, -7.9066e-02,  3.7292e-01,\n",
       "         -3.8754e-01, -2.6918e-01, -1.6587e-01, -9.5711e-01, -2.9138e-01,\n",
       "          9.5930e-01, -2.4220e-01, -3.4730e-01,  5.5905e-01, -2.2169e-01,\n",
       "         -4.4686e-01,  8.5398e-01,  2.9005e-01,  3.8162e-01, -1.3793e-01,\n",
       "          4.4722e-01, -3.4301e-02,  4.6935e-01, -8.3090e-01,  2.0557e-01,\n",
       "          3.0258e-01, -2.3434e-01, -4.2814e-01, -9.8585e-01, -2.6726e-01,\n",
       "          4.1528e-01,  9.9101e-01,  7.7667e-01,  2.4347e-01,  4.1400e-01,\n",
       "         -1.8944e-01,  4.9667e-01, -9.6334e-01,  9.8587e-01, -1.9165e-01,\n",
       "          2.8567e-01, -2.9670e-01,  2.3316e-01, -8.2953e-01, -1.0146e-01,\n",
       "          8.3441e-01, -5.6830e-01, -8.0793e-01,  3.8112e-02, -3.9030e-01,\n",
       "         -3.4548e-01, -5.9944e-01,  3.8475e-01, -3.4689e-01, -4.3267e-01,\n",
       "         -7.5528e-02,  9.5030e-01,  9.4135e-01,  7.9525e-01, -3.0725e-01,\n",
       "          5.1429e-01, -9.2341e-01, -3.5159e-01,  1.3071e-01,  1.6805e-01,\n",
       "          1.6153e-01,  9.9501e-01, -4.0927e-01, -7.7082e-02, -9.4992e-01,\n",
       "         -9.8991e-01, -1.1422e-01, -8.7494e-01,  1.2509e-03, -6.1494e-01,\n",
       "          4.9158e-01, -2.5104e-01, -2.0968e-02,  4.1147e-01, -9.7516e-01,\n",
       "         -7.9317e-01,  3.5086e-01, -3.2759e-01,  4.4569e-01, -2.0290e-01,\n",
       "          6.9001e-01,  8.0891e-01, -5.5403e-01,  5.0323e-01,  9.0446e-01,\n",
       "         -5.2136e-01, -8.0340e-01,  7.4889e-01, -2.7407e-01,  8.2888e-01,\n",
       "         -6.7940e-01,  9.8790e-01,  5.9197e-01,  4.4886e-01, -9.5243e-01,\n",
       "         -2.5408e-01, -8.2430e-01, -2.3641e-01, -9.8606e-02, -3.8590e-01,\n",
       "          4.1045e-01,  4.6275e-01,  3.4113e-01,  7.7696e-01, -6.4979e-01,\n",
       "          9.9502e-01, -8.3080e-01, -9.6256e-01, -6.6545e-01, -1.7488e-01,\n",
       "         -9.9117e-01,  6.8051e-01,  3.4636e-01,  2.5930e-02, -4.0094e-01,\n",
       "         -5.7990e-01, -9.6441e-01,  8.4064e-01,  9.7404e-02,  9.7685e-01,\n",
       "         -2.2898e-01, -9.0465e-01, -4.9228e-01, -9.3946e-01, -2.1256e-01,\n",
       "         -1.1724e-01,  3.6842e-01, -1.6674e-01, -9.7718e-01,  4.4888e-01,\n",
       "          5.2090e-01,  4.7166e-01, -1.0468e-01,  9.9669e-01,  1.0000e+00,\n",
       "          9.7964e-01,  8.8182e-01,  8.6335e-01, -9.9678e-01, -6.7539e-01,\n",
       "          9.9999e-01, -9.2170e-01, -1.0000e+00, -9.2834e-01, -5.9170e-01,\n",
       "          2.3276e-01, -1.0000e+00, -9.8812e-02,  9.0308e-03, -9.3336e-01,\n",
       "          5.1926e-02,  9.7718e-01,  9.8588e-01, -1.0000e+00,  8.6395e-01,\n",
       "          9.4508e-01, -5.3205e-01,  7.3600e-01, -4.9264e-01,  9.7720e-01,\n",
       "          2.7135e-01,  5.0529e-01, -2.5390e-01,  3.8093e-01, -6.6860e-01,\n",
       "         -8.4742e-01,  3.9355e-02, -3.3288e-01,  9.6795e-01,  1.2675e-01,\n",
       "         -7.0034e-01, -9.4892e-01,  3.2911e-01, -7.5299e-02, -8.4119e-02,\n",
       "         -9.6886e-01, -1.7793e-01,  2.7686e-01,  7.0191e-01,  1.2640e-01,\n",
       "          2.8888e-01, -7.5103e-01,  1.7132e-01, -5.7487e-01,  3.9643e-01,\n",
       "          6.0315e-01, -9.4434e-01, -5.3384e-01,  2.3952e-01, -3.7357e-01,\n",
       "         -1.2886e-01, -9.5059e-01,  9.7152e-01, -4.7304e-01,  5.3348e-01,\n",
       "          1.0000e+00,  4.1599e-01, -8.9661e-01,  4.6612e-01,  1.8370e-01,\n",
       "         -1.2100e-01,  1.0000e+00,  6.7526e-01, -9.8438e-01, -5.0058e-01,\n",
       "          6.5622e-01, -4.4368e-01, -4.6393e-01,  9.9939e-01, -1.7388e-01,\n",
       "          8.1112e-04,  1.6268e-01,  9.8249e-01, -9.9291e-01,  9.2047e-01,\n",
       "         -8.7973e-01, -9.8108e-01,  9.7064e-01,  9.5401e-01, -3.9329e-01,\n",
       "         -7.0517e-01,  1.1986e-01, -1.9687e-01,  2.3772e-01, -9.3961e-01,\n",
       "          6.7438e-01,  3.3678e-01, -6.2104e-02,  9.1531e-01, -6.9753e-01,\n",
       "         -4.2231e-01,  2.8740e-01, -8.2716e-02,  2.0071e-01,  7.5623e-01,\n",
       "          5.5498e-01, -2.6112e-01, -1.4742e-02, -2.8436e-01, -5.0740e-01,\n",
       "         -9.7238e-01,  2.1581e-01,  1.0000e+00, -1.7578e-01,  3.5664e-01,\n",
       "         -1.5466e-01, -4.7785e-02, -1.8258e-01,  3.9500e-01,  4.3494e-01,\n",
       "         -3.1701e-01, -8.7822e-01,  5.4911e-01, -9.2405e-01, -9.9261e-01,\n",
       "          7.1431e-01,  1.7870e-01, -2.2714e-01,  9.9994e-01,  3.7316e-01,\n",
       "          2.1984e-01,  1.1093e-01,  8.0245e-01, -8.0960e-02,  3.8758e-01,\n",
       "          1.4727e-01,  9.7841e-01, -2.0909e-01,  4.1623e-01,  7.7878e-01,\n",
       "         -2.8315e-01, -2.7102e-01, -6.7629e-01,  5.4102e-02, -9.2375e-01,\n",
       "          1.7734e-01, -9.6716e-01,  9.6515e-01,  5.2673e-01,  3.5246e-01,\n",
       "          1.5233e-01,  3.6619e-01,  1.0000e+00, -6.5872e-01,  4.8457e-01,\n",
       "          1.9578e-01,  7.3940e-01, -9.9210e-01, -7.4739e-01, -4.2390e-01,\n",
       "          7.6736e-02, -2.0375e-01, -2.4413e-01,  2.0446e-01, -9.7110e-01,\n",
       "          1.0065e-02,  3.7053e-01, -9.7850e-01, -9.9453e-01,  6.6104e-02,\n",
       "          6.2390e-01,  9.1608e-02, -8.8701e-01, -6.9612e-01, -5.7321e-01,\n",
       "          4.8195e-01, -1.7813e-01, -9.5121e-01,  4.8076e-01, -2.4236e-01,\n",
       "          4.5844e-01, -1.6935e-01,  4.9598e-01,  9.2124e-02,  8.2561e-01,\n",
       "         -2.5844e-01, -9.4820e-02, -1.0803e-01, -7.3211e-01,  7.0926e-01,\n",
       "         -7.6118e-01, -7.4970e-01, -4.4924e-02,  1.0000e+00, -3.6870e-01,\n",
       "          4.0801e-01,  7.2361e-01,  5.6285e-01, -1.1088e-01,  1.2329e-01,\n",
       "          7.2663e-01,  2.6733e-01, -1.8730e-01,  6.6897e-02, -2.4701e-01,\n",
       "         -3.4579e-01,  4.6110e-01,  1.3864e-01,  1.2371e-01,  8.1839e-01,\n",
       "          5.5027e-01,  6.2720e-02,  1.5292e-01, -2.9001e-02,  9.9868e-01,\n",
       "         -1.8245e-01, -2.7669e-01, -4.2753e-01, -5.5571e-02, -2.8526e-01,\n",
       "          1.9444e-01,  1.0000e+00,  2.9937e-01,  2.7396e-01, -9.9326e-01,\n",
       "         -5.5501e-01, -9.1143e-01,  9.9998e-01,  7.9874e-01, -7.9978e-01,\n",
       "          5.6606e-01,  5.8583e-01, -3.1645e-02,  6.8266e-01, -1.6277e-01,\n",
       "         -2.2530e-01,  3.1238e-01,  1.4621e-01,  9.6970e-01, -4.5824e-01,\n",
       "         -9.7580e-01, -5.1239e-01,  3.6515e-01, -9.6763e-01,  9.9808e-01,\n",
       "         -4.7724e-01, -2.7172e-01, -3.6759e-01, -3.5320e-01, -1.1973e-01,\n",
       "          5.3206e-02, -9.8527e-01, -1.8769e-01,  6.3142e-02,  9.7282e-01,\n",
       "          2.0451e-01, -4.8211e-01, -8.8745e-01,  3.6189e-01,  1.1808e-01,\n",
       "         -4.5706e-01, -9.7204e-01,  9.7895e-01, -9.8342e-01,  6.1600e-01,\n",
       "          1.0000e+00,  1.8808e-01, -3.7024e-01,  1.2083e-01, -3.4058e-01,\n",
       "          3.2699e-01, -4.6879e-01,  4.8294e-01, -9.6377e-01, -2.2506e-01,\n",
       "         -2.1618e-01,  3.2090e-01, -1.1245e-01, -5.7939e-01,  7.7800e-01,\n",
       "          8.0418e-02, -4.3637e-01, -5.7621e-01, -3.5239e-02,  3.4736e-01,\n",
       "          7.6027e-01, -3.2007e-01, -8.2203e-02,  3.1388e-02,  3.1223e-02,\n",
       "         -9.5815e-01, -3.2892e-01, -3.0332e-01, -9.9956e-01,  5.5293e-01,\n",
       "         -1.0000e+00,  2.4952e-01, -3.2146e-01, -1.2790e-01,  8.4925e-01,\n",
       "          4.6197e-01,  3.1184e-01, -7.9114e-01, -4.6068e-02,  8.2763e-01,\n",
       "          7.9372e-01, -1.7230e-01, -2.1722e-01, -7.8770e-01,  2.4512e-01,\n",
       "         -4.8330e-02,  4.2679e-01, -2.6726e-01,  7.1404e-01, -1.6525e-01,\n",
       "          1.0000e+00, -5.0127e-02, -3.8291e-01, -9.6242e-01,  2.1287e-01,\n",
       "         -2.2878e-01,  1.0000e+00, -8.2347e-01, -9.5603e-01,  4.9371e-01,\n",
       "         -5.1263e-01, -8.1639e-01,  3.5393e-01, -1.2361e-01, -7.1341e-01,\n",
       "         -6.2359e-01,  9.6443e-01,  6.7158e-01, -4.9004e-01,  5.7314e-01,\n",
       "         -1.9505e-01, -4.3617e-01, -5.1266e-02,  5.7459e-01,  9.9066e-01,\n",
       "          5.3986e-01,  8.4048e-01, -2.4398e-01, -7.6392e-02,  9.6860e-01,\n",
       "          1.6377e-01,  5.3132e-01,  1.3963e-01,  1.0000e+00,  2.7477e-01,\n",
       "         -9.0756e-01,  2.3490e-01, -9.8173e-01, -1.1559e-01, -9.3482e-01,\n",
       "          3.0709e-01,  1.4538e-01,  9.0642e-01, -1.7776e-01,  9.6802e-01,\n",
       "         -3.8933e-01,  2.2264e-02, -3.2779e-01,  2.8648e-01,  4.5049e-01,\n",
       "         -9.4169e-01, -9.8785e-01, -9.9012e-01,  4.5042e-01, -4.7485e-01,\n",
       "          5.1656e-02,  2.6218e-01,  2.6494e-02,  4.1697e-01,  4.3559e-01,\n",
       "         -1.0000e+00,  9.4521e-01,  3.7314e-01,  4.0605e-01,  9.7477e-01,\n",
       "          5.1287e-01,  4.7573e-01,  2.6314e-01, -9.8814e-01, -9.7092e-01,\n",
       "         -3.7781e-01, -2.2812e-01,  7.9339e-01,  6.7028e-01,  8.6433e-01,\n",
       "          4.4965e-01, -4.3954e-01, -3.4842e-01,  9.0778e-02, -7.8669e-01,\n",
       "         -9.9419e-01,  3.2891e-01,  1.1429e-01, -9.0991e-01,  9.6858e-01,\n",
       "         -5.2829e-01, -7.8307e-02,  3.7428e-01, -5.0130e-01,  8.9134e-01,\n",
       "          8.1356e-01,  3.6603e-01,  1.8666e-01,  4.5219e-01,  9.1263e-01,\n",
       "          9.2888e-01,  9.9164e-01, -4.3408e-01,  7.1544e-01, -2.0429e-01,\n",
       "          4.5715e-01,  6.5803e-01, -9.4560e-01,  9.3201e-02,  1.4068e-01,\n",
       "         -2.5250e-01,  2.3494e-01, -1.8436e-01, -9.1847e-01,  7.0895e-01,\n",
       "         -1.5676e-01,  5.2854e-01, -4.3283e-01,  9.8896e-02, -3.8200e-01,\n",
       "         -1.6871e-01, -6.9213e-01, -5.0607e-01,  5.1501e-01,  1.2895e-01,\n",
       "          8.9616e-01,  8.1350e-01, -4.1092e-03, -5.9981e-01, -2.0251e-01,\n",
       "         -1.7873e-01, -9.2477e-01,  8.5841e-01, -2.2853e-02,  1.3030e-01,\n",
       "          5.3568e-02,  1.3016e-02,  9.1883e-01, -1.0836e-01, -3.4677e-01,\n",
       "         -2.6193e-01, -6.5729e-01,  8.9785e-01, -3.2738e-01, -4.7887e-01,\n",
       "         -5.1768e-01,  8.0648e-01,  3.5052e-01,  9.9947e-01, -3.5897e-01,\n",
       "         -4.1162e-01, -2.5608e-01, -3.3218e-01,  3.1261e-01, -4.4002e-01,\n",
       "         -1.0000e+00,  4.4778e-01, -2.4689e-01,  3.6872e-01, -1.8791e-01,\n",
       "          4.2501e-01, -2.4418e-01, -9.6904e-01, -2.8277e-01,  1.6994e-01,\n",
       "          2.2197e-02, -4.7214e-01, -5.2405e-01,  4.7511e-01, -2.8969e-01,\n",
       "          8.6675e-01,  9.0030e-01,  2.4527e-01,  6.6713e-01,  5.1664e-01,\n",
       "         -8.6173e-02, -7.2003e-01,  9.1340e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bbb56409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1778,  0.0946, -0.0306,  ..., -0.3201,  0.3875,  0.3578],\n",
       "         [ 0.6455,  0.4590,  0.4842,  ..., -0.5112,  0.7038, -0.3384],\n",
       "         [-0.0900, -0.4080,  0.7350,  ..., -1.0433, -0.1517, -0.2731],\n",
       "         ...,\n",
       "         [ 0.1477,  0.0171,  0.6067,  ..., -0.4519,  0.1851, -0.3535],\n",
       "         [-0.2638, -0.2562,  0.0473,  ...,  0.2247,  0.3425, -0.3819],\n",
       "         [ 0.5196, -0.0754, -0.2657,  ..., -0.0014, -0.4388, -0.2921]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding for each token, the first one being the [CLS] token\n",
    "\n",
    "response.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb51481d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This layer is trained on top of the embedding of [CLS] token\n",
    "\n",
    "response.pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99eabe9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertPooler(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pooler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1cf6ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLS_embedding = response.last_hidden_state[:,0,:].unsqueeze(0)\n",
    "CLS_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21fcc048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.pooler(CLS_embedding).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f517045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the embedding for CLS through the pooler gives the same output as the 'pooler_output'\n",
    "\n",
    "(model.pooler(CLS_embedding) == response.pooler_output).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f1970c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 109,360,128\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "\n",
    "for p in model.parameters():\n",
    "    if len(p.shape) == 2:\n",
    "        total_params += p.shape[0] * p.shape[1]\n",
    "        \n",
    "print(f'Total Parameters: {total_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881c0e38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
